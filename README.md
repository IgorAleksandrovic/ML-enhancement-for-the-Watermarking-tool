# ML-enhancement-for-the-Watermarking-tool
Final project for the Building AI course
Enhancement of the current watermarking tool https://github.com/IgorAleksandrovic/Block-based-DWT-DTC-watermarking.git using ML techniques in order to optimize watermarking parameters on a per image basis

## Summary

An intelligent digital image watermarking system that uses machine learning to automatically optimize watermarking parameters for minimal visibility while maintaining robustness. The system combines traditional DWT/DCT watermarking with logistic regression to predict optimal embedding parameters for each image in real-time.

## Background

Digital image watermarking faces a fundamental trade-off between robustness (watermark survival) and imperceptibility (visual quality). Current watermarking systems use fixed parameters that work well for some images but may be too strong (visible artifacts) or too weak (poor robustness) for others. Manual parameter tuning is time-consuming and impractical for real-world applications.

The current implementation https://github.com/IgorAleksandrovic/Block-based-DWT-DTC-watermarking.git is robust against standard image transformations like compression and resizing, but suffers from visibility issues in certain image types (smooth gradients, low-texture areas, specific luminance ranges). Manual adjustment of the numerous parameters (strength, luminance scales, texture weights, sub-band weights) is inefficient and doesn't scale for diverse image datasets.

Key problems this AI enhancement solves:
* Automatic parameter optimization eliminating manual tuning
* Adaptive watermarking that considers individual image characteristics
* Consistent imperceptibility across diverse image types
* Real-time optimization suitable for production environments

## How is it used?

The AI-enhanced watermarking system operates in two phases: training and deployment.

**Training Phase:** The system learns optimal parameter combinations from a diverse dataset of images with known visibility outcomes.

**Deployment Phase:** For each new image, the AI model analyzes image characteristics and predicts optimal watermarking parameters before embedding.

### Usage Process:

1. **Image Analysis**: Extract perceptual features (luminance distribution, texture measures, edge density)
2. **Parameter Prediction**: AI model predicts optimal embedding parameters
3. **Adaptive Watermarking**: Apply watermark using predicted parameters
4. **Verification**: Standard block-based verification process

### Target Users:
- Content creators protecting digital artwork
- News organizations ensuring image authenticity
- Social media platforms implementing content attribution
- Digital forensics investigators
- Copyright protection services

The system is particularly valuable in high-volume scenarios where manual parameter adjustment is impractical, such as automated content management systems or real-time image processing pipelines.

### Integration Example:
```python
from ai_parameter_optimizer import ParameterOptimizer
from image_watermarking import ImageSigner

# Initialize AI-enhanced signer
optimizer = ParameterOptimizer('trained_model.pkl')
signer = ImageSigner(secret_key='your_key')

# AI automatically optimizes parameters
optimal_params = optimizer.predict_parameters('input.jpg')
signer.update_parameters(optimal_params)

# Apply watermark with optimized settings
signature = signer.embed_watermark('input.jpg', 'output.jpg')
```

## Data sources and AI methods

### AI Method: Logistic Regression

Logistic regression was selected as the optimal machine learning approach for this watermarking parameter optimization task. Unlike linear regression, logistic regression can model the binary decision boundary between "imperceptible" and "visible" watermarking, while being fast enough for real-time operation. The model predicts the probability that a given parameter set will produce imperceptible watermarking for a specific image's characteristics.

### Dataset Design and Generation

The training dataset consists of image-parameter-outcome triplets generated through systematic experimentation:

**Dataset Structure:**
```
Image Features (Input):
- Luminance statistics (mean, std, histogram bins)
- Texture complexity measures (local variance, gradient magnitude)
- Edge density and orientation
- Frequency domain characteristics
- Spatial activity measures

Parameter Combinations (Input):
- Embedding strength values
- Luminance scale factors  
- Texture weight parameters
- Sub-band weight distributions

Imperceptibility Labels (Output):
- Binary classification: 0 (visible artifacts) or 1 (imperceptible)
- Based on perceptual metrics (PSNR > 40dB, SSIM > 0.95, LPIPS < 0.1)
- Human evaluation validation on subset
```

**Dataset Generation Process:**

1. **Image Collection**: Curate 5,000+ diverse images across categories:
   - Natural scenes (landscapes, portraits, objects)
   - Synthetic images (graphics, text, diagrams)  
   - Various lighting conditions and textures
   - Different resolutions and aspect ratios

2. **Parameter Space Exploration**: For each image, test systematic combinations:
   - Strength: [1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0]
   - Luminance scale: [64, 96, 128, 160, 192]
   - Texture scales: [20, 30, 40, 50]
   - Generate ~150 parameter combinations per image

3. **Quality Assessment**: 
   - Apply each parameter combination
   - Measure perceptual quality using PSNR, SSIM, and LPIPS metrics
   - Label as imperceptible (1) if all metrics exceed thresholds
   - Verify robustness through standard attacks (JPEG compression, resizing)

4. **Feature Extraction**: Extract standardized image characteristics:
   - Statistical moments of luminance distribution
   - Local binary pattern histograms for texture
   - Gradient-based edge measurements
   - DWT coefficient energy distributions

**Expected Dataset Size**: ~750,000 training samples (5,000 images × 150 parameter combinations)

### Model Training and Validation

The logistic regression model uses L2 regularization to prevent overfitting and ensure generalization across diverse image types. Cross-validation ensures robust performance across different image categories.

## AI Integration Architecture

The AI enhancement integrates seamlessly with the existing watermarking system through a dedicated parameter optimization module:

**File Structure:**
```
project/
├── image_watermarking.py      # Core DWT/DCT watermarking (existing)
├── ImgSignBlock.py           # CLI interface (existing)  
├── ai_parameter_optimizer.py # NEW: AI parameter prediction
├── dataset_generator.py      # NEW: Training data creation
├── model_trainer.py          # NEW: Model training pipeline
└── trained_models/           # NEW: Saved model files
    └── parameter_optimizer.pkl
```

**Integration Point:** The AI module (`ai_parameter_optimizer.py`) is called by the `ImageSigner` class before watermark embedding to predict optimal parameters based on input image analysis.

## Challenges

**Current Limitations:**
- Model training requires substantial computational resources for dataset generation
- Performance depends on training data diversity - may not generalize to completely novel image types
- Real-time prediction adds minimal computational overhead but requires model loading
- Binary classification may not capture nuanced visibility perceptions

**Ethical Considerations:**
- Watermarking strength optimization should not enable creation of completely undetectable watermarks that could facilitate copyright infringement
- Training data must represent diverse image types to avoid algorithmic bias
- Model decisions should remain interpretable for forensic applications

**Technical Constraints:**
- Model size must remain small enough for real-time deployment
- Parameter predictions must maintain backward compatibility with existing verification systems
- Robustness evaluation requires standardized attack protocols

## What next?

**Immediate Enhancements:**
- Implement multi-objective optimization to balance imperceptibility, robustness, and embedding capacity
- Develop confidence scores for parameter predictions to flag uncertain cases
- Create web-based interface for interactive parameter optimization
- Implement more sophisticated per-block parameter optimisation

**Advanced AI Integration:**
- Neural network approaches for more complex feature learning
- Reinforcement learning for adaptive parameter adjustment based on attack detection
- Transfer learning for domain-specific optimization (medical images, artistic content)

**Skills and Assistance Needed:**
- Computer vision expertise for advanced perceptual feature engineering
- Human-computer interaction research for perceptual quality evaluation
- Cybersecurity knowledge for comprehensive robustness testing
- Cloud infrastructure for scalable model training and deployment

**Potential Applications:**
- Integration with content management systems
- Mobile app development for real-time photo protection
- Enterprise-level batch processing solutions
- Blockchain integration for decentralized content verification

## Acknowledgments

* Original DWT-DCT watermarking implementation based on research by Al-Haj (2007) and recent advances in perceptual watermarking
* Perceptual quality metrics implementation using established computer vision libraries
* Machine learning framework built on scikit-learn for accessibility and reproducibility
* Training dataset methodology inspired by image quality assessment research
* Special thanks to the Building AI course community for feedback and testing support
